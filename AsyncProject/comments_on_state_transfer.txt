COMMENTS ON STATE TRANSFER [2017-09-19 added this section]

when checkpointing is used, during reconfiguration, the running state (of the object) needs to be sent to the new replicas in the new configuration, because the running state cannot be computed from the truncated histories and the checkpoint proofs.  Ho's thesis does not directly address this, because it considers state transfer before introducing checkpointing.  there are a variety of ways to handle this.  here is a proposal, similar to the history-based state transfer protocol in Ho's thesis (section 5.4.9).

let Q be a quorum of replicas of the old configuration C from which olympus has received consistent valid histories.  let w be the set of wedged messages received from the replicas in Q.  let w(rho) be the wedged message from replica rho in Q.  for a wedged message m, let m.history, m.checkptProof, etc., denote the components of the message.  "consistent" means that, for each pair of replicas rho1 and rho2 in Q, for each slot s for which an order proof appears in w(rho1).history and w(rho2).history, the order proofs for s are consistent, i.e., are for the same operation.  this implies that, if we think of each history in w as defining a sequence of <slot, operation> pairs, and if we let LH (for "longest history") denote the longest of these sequences, then all of the other such sequences are prefixes of LH.  in other words, all replicas agree on the slots that their histories have in common, and the only difference is that some replicas (i.e., those closer the the head) may be are farther along than others, i.e., may have ordered more operations.  if the histories are not consistent, then one of the replicas in Q is faulty, and Olympus needs to choose a different quorum.  note that there might be multiple replicas that sent equally long sequences, or there might be only one.

[2017-11-16 added] note: if a replica has not yet received a completed checkpoint proof for its most recent checkpoint, it should sent to Olympus its history starting from the previous checkpoint, and the completed checkpoint proof for that prevous checkpoint.  Olympus will decide whether to start the catch-up history from that checkpoint or the most recent checkpoint, depending on the messages it receives from other replicas.

it is tempting to simply copy the running state from some replica rho_L whose wedged message contains the longest history LH.  however, this is unsafe, because rho_L could be faulty and lie about its running state, while sending correct information about everything else.  to ensure safety, we need supporting evidence for the correctness of the running state from every replica in the quorum.  to obtain this evidence, Olympus asks every replica rho in Q to "catch up" to LH and then send Olympus a cryptographic hash of its running state.  in more detail, Olympus sends each replica rho in Q a "catch_up" message containing the sequence of operations catch_up(rho) = LH - w(rho).history (this is the suffix of LH that rho has not executed yet; it may be the empty sequence).  upon receiving this message, rho executes those operations and then computes and sends a cryptographic hash of its resulting running state to Olympus in a "caught_up" message.  Olympus checks that it receives the same cryptographic hash ch in the caught_up messages from all replicas in Q.  if not, one of the replicas in Q is faulty, and Olympus needs to choose a different quorum.

[2017-11-13 added] we augment the reconfiguration algorithm as follows to try to ensure that a result shuttle is sent to each client for the client's last request executed in the old configuration.  notes: (1) clients ignore redundant results, and the cost of sending a few redundant results during reconfiguration is negligible, especially since reconfiguration is rare; (2) it is sufficient to consider the last request from each client, because the client must have received responses to its earlier requests; (3) this is necessary partly because the result cache is not transferred during reconfiguration; (3) this increases the probability that clients receive a result for each request but does not guarantee it; if we wanted to provide a guarantee, we should have adopted the strong approach to client request identififiers (cf. COMMENTS ON REQUEST IDENTIFIERS).

[2017-11-13 added] in caught_up messages, each replica rho includes, for each client c, rho's result statement for the most recently executed request from c.  rho may have executed this request during normal processing before reconfiguration or during catch-up.  note that this requires each replica to cache its most recent result statement for each client.  for each client c for which Olympus receives (in the caught_up messages) a quorum of consistent result statements for the same request from c, Olympus sends to c a result shuttle containing those result statements.

now Olympus sends a "get_running_state" message to some replica rho in Q (it doesn't matter which one), and rho replies with a "running_state" message containing its running state S.  Olympus computes the cryptographic hash of S and compares it with ch.  if they differ, Olympus requests the running state from another replica in Q.  if they are equal, Olympus includes S in its inithist message as the initial running state of the new configuration.  note that each replica in the new configuration initializes its history to be the empty sequence.

note: validation of the checkpoint proof during reconfiguration is included to guide selection of the checkpoint used as the starting point for catch-up using the longest consistent history LH.  for example, if checkpoint is taken every 100 slots, and Olympus receives a valid complete checkpoint proof for slot 100, and only incomplete or invalid checkpoint proofs for slot 200, then it should use slot 100 as the starting point of LH.  I am not certain that validation of the checkpoint proof is necessary here.  in the absence of being certain that it is unnecessary, it is safer to include it.

note: if the running state is large, this approach can be optimized as follows. Olympus asks some replica rho in Q to send its running state directly to the replicas in the new configuration.  Olympus includes ch (not the entire running state) in its inithist message.  the replicas in the new configuration validate the running state received from the old replica by computing its crytographic hash and comparing it to ch.  with this optimization, we need to worry about rho failing during this process.  this introduces some complications, so we won't bother with it.

note: advantages of this approach are (1) a replica does not need to create and save a copy of its running state when it takes a checkpoint, and (2) Olympus never applies operations to update a running state (e.g., if the running state is in a DBMS, Olympus does not need to run a copy of the DBMS).  if we require replicas to save their state at checkpoints, and if we assume that Olympus can apply operations to update the state, then the following simpler protocol is possible.  each replica includes its most recent saved checkpoint state in its wedged message.  Olympus computes the cryptographic hashes of these states to check that they are consistent with the crytographic hash in the checkpoint proof.  Olympus brings this checkpoint state up-to-date by applying all of the operations in LH, and uses the resulting state as the initial running state of the new configuration.  note that histories in wedged messages in w contain only operations applied after the last checkpoint, so Olympus applies all operations in LH to bring the checkpoint state up-to-date.

COMMENTS ON CLIENT SIGNATURES [2017-11-12]

The shuttle should include the client's entire signed request, and each replica should check that the request has a valid client signature.  This prevents the head from fabricating requests.  This is implied by the last paragraph in Section 3.4.  

[2017-11-13] During reconfiguration, Olympus should also check signatures on client requests.

COMMENT ON PROGRESS WITH FAULTY REPLICAS [2017-11-12]

it is interesting to compare Raft and BCR in terms of progress with faulty replicas.

Raft can make progress (i.e., process client requests) without reconfiguring the system to remove faulty replicas.  In most cases, BCR cannot make progress with any faulty replica in the configuration; BCR needs to reconfigure to remove faulty replicas before it can make progress.  note that the system may reconfigure to a smaller size [although we won't do this in the project].  for example, consider BCR with t=1 and n=3 (n is the number of replicas).  suppose a replica fails and is removed, leading to a system with n=2.  the reconfigured system can make progress, because t is still 1 and quorums still have size t+1 = 2.

======================================================================
SIGNATURES

your pseudocode and implementation should deviate from the description of HMAC Shuttle in the paper by using public-key cryptography, instead of vectors of HMACs, for all signatures by clients and replicas.  furthermore, Olympus can create all public and private keys for clients and replicas and send them to the relevant processes.  this implies that you do not need to use the Diffie-Hellman key-exchange protocol mentioned in section 4.1.